{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c67bc2a",
   "metadata": {},
   "source": [
    "## Stage 4: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca356044",
   "metadata": {},
   "source": [
    "### 1. Objective\n",
    "We prepare the data to be feed to the training stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31047bf2",
   "metadata": {},
   "source": [
    "### 2. Approach\n",
    "Suing the library tensorflow, we preprocess the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714e2ba",
   "metadata": {},
   "source": [
    "### 3. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93956764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.features.padding import padding_func\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618cecca",
   "metadata": {},
   "source": [
    "Like we mentioned in the previous stage, we're going to use 'Observaciones' for sentences and 'Especialidad' for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9e168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75866 entries, 0 to 75865\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Observaciones  75866 non-null  object\n",
      " 1   Especialidad   75866 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "current_dir = os.getcwd()\n",
    "dataset_path = os.path.join(current_dir, '..', 'data/raw', 'close.csv')\n",
    "data_raw = pd.read_csv(dataset_path, low_memory=False, usecols=['Observaciones', 'Especialidad'])\n",
    "# Basic statistics\n",
    "print(data_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0cce8",
   "metadata": {},
   "source": [
    "The labels with more than 300 entries stay as they are, and the rest are grouped in 'OTHERS'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc405e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Especialidad_groped\n",
      "8 - MECANICA                       24477\n",
      "1 - ELECTRICIDAD                   16674\n",
      "12 - ELECTROMEDICINA                9048\n",
      "5 - FONTANERIA                      8910\n",
      "4 - CARPINTERIA                     5318\n",
      "3 - CALEFACCIÓN Y CLIMATIZACIÓN     3801\n",
      "19 - APOYO NO ESPECIALIZADO         3089\n",
      "7 - ALBAÑILERIA                     1704\n",
      "17 - VARIAS ESPECIALIDADES          1140\n",
      "6 - PINTURA Y REVESTIMIENTOS         891\n",
      "OTHERS                               435\n",
      "13 - RED DE VOZ Y DATOS              379\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_label = data_raw['Especialidad'].value_counts().index.where(data_raw['Especialidad'].value_counts() >= 300)\n",
    "data_raw['Especialidad_groped'] = data_raw['Especialidad'].where(data_raw['Especialidad'].isin(top_label), 'OTHERS')\n",
    "print(data_raw['Especialidad_groped'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad9e363",
   "metadata": {},
   "source": [
    "Split the data in training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8ea883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 60693\n",
      "Test data size: 15173\n"
     ]
    }
   ],
   "source": [
    "training_data = data_raw.sample(frac=0.8, random_state=42)\n",
    "test_data = data_raw.drop(training_data.index)\n",
    "training_text, training_label = training_data['Observaciones'], training_data['Especialidad_groped']\n",
    "test_text, test_label = test_data['Observaciones'], test_data['Especialidad_groped']\n",
    "training_text.to_csv(os.path.join(current_dir, '..', 'data/processed', 'training_text.csv'), index=False)\n",
    "test_text.to_csv(os.path.join(current_dir, '..', 'data/processed', 'test_text.csv'), index=False)\n",
    "training_label.to_csv(os.path.join(current_dir, '..', 'data/processed', 'training_label.csv'), index=False)\n",
    "test_label.to_csv(os.path.join(current_dir, '..', 'data/processed', 'test_label.csv'), index=False)\n",
    "print(f\"Training data size: {len(training_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb535b",
   "metadata": {},
   "source": [
    "Check how many entries for each label in test and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "889b8d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Training  Test\n",
      "Especialidad_groped                            \n",
      "8 - MECANICA                        19607  4870\n",
      "1 - ELECTRICIDAD                    13332  3342\n",
      "12 - ELECTROMEDICINA                 7195  1853\n",
      "5 - FONTANERIA                       7129  1781\n",
      "4 - CARPINTERIA                      4233  1085\n",
      "3 - CALEFACCIÓN Y CLIMATIZACIÓN      3039   762\n",
      "19 - APOYO NO ESPECIALIZADO          2480   609\n",
      "7 - ALBAÑILERIA                      1370   334\n",
      "17 - VARIAS ESPECIALIDADES            915   225\n",
      "6 - PINTURA Y REVESTIMIENTOS          733   158\n",
      "OTHERS                                356    79\n",
      "13 - RED DE VOZ Y DATOS               304    75\n"
     ]
    }
   ],
   "source": [
    "# Check how many entries for each label in test and training in two column to compare\n",
    "label_counts = pd.DataFrame({\n",
    "    \"Training\": training_label.value_counts(),\n",
    "    \"Test\": test_label.value_counts()\n",
    "}).fillna(0)\n",
    "\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b642d",
   "metadata": {},
   "source": [
    "Let's start vectorization with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "781b500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 [UNK]\n",
      "2 de\n",
      "3 la\n",
      "4 en\n",
      "5 gracias\n",
      "6 y\n",
      "7 el\n",
      "8 del\n",
      "9 no\n",
      "Vocabulary size: 22386\n"
     ]
    }
   ],
   "source": [
    "# Select sentences for vectorization\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(ragged=True)\n",
    "vectorize_layer.adapt(training_text)\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "# Display the first 10 words in the vocabulary\n",
    "for index, word in enumerate(vocabulary[:10]):\n",
    "    print(index, word)\n",
    "print(f'Vocabulary size: {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15ebc4",
   "metadata": {},
   "source": [
    "The total of different words of vocabulary is 22386. To simplify the training and model, we're going to limit the vocabulary to 20000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f461179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20000\n"
     ]
    }
   ],
   "source": [
    "# Select sentences for vectorization\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(ragged=True, max_tokens=20000)\n",
    "vectorize_layer.adapt(training_text)\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "# Display the first 10 words in the vocabulary\n",
    "print(f'Vocabulary size: {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8188ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequences shape: <_TensorSliceDataset element_spec=TensorSpec(shape=(120,), dtype=tf.int32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "train_sequences = padding_func(vectorize_layer(training_text))\n",
    "test_sequences = padding_func(vectorize_layer(test_text))\n",
    "print(f'Train sequences shape: {train_sequences}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa6b65",
   "metadata": {},
   "source": [
    "Let's vectorized the labels too. One-hot labeling it's gonna be useful later for F1Score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaea138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example label mapping: [('12 - ELECTROMEDICINA', 0), ('1 - ELECTRICIDAD', 1), ('8 - MECANICA', 2), ('3 - CALEFACCIÓN Y CLIMATIZACIÓN', 3), ('7 - ALBAÑILERIA', 4), ('5 - FONTANERIA', 5), ('19 - APOYO NO ESPECIALIZADO', 6), ('4 - CARPINTERIA', 7), ('17 - VARIAS ESPECIALIDADES', 8), ('OTHERS', 9), ('13 - RED DE VOZ Y DATOS', 10), ('6 - PINTURA Y REVESTIMIENTOS', 11)]\n",
      "First 5 y_train (one-hot): [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the label dataset: convert string labels to integer indices and one-hot encode\n",
    "\n",
    "label_names = training_label.unique().tolist()\n",
    "label_to_index = {name: idx for idx, name in enumerate(label_names)}\n",
    "# Save the label mapping\n",
    "label_mapping_path = os.path.join(current_dir, '..', 'data/processed', 'label_mapping.csv')\n",
    "pd.DataFrame(list(label_to_index.items()), columns=['label', 'index']).to_csv(label_mapping_path, index=False)\n",
    "\n",
    "y_train_int = training_label.map(label_to_index).values\n",
    "y_test_int = test_label.map(label_to_index).values\n",
    "\n",
    "# # One-hot encode the integer labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train_int, num_classes=len(label_names))\n",
    "y_test = tf.keras.utils.to_categorical(y_test_int, num_classes=len(label_names))\n",
    "\n",
    "print(f\"Example label mapping: {list(label_to_index.items())}\")\n",
    "print(f\"First 5 y_train (one-hot): {y_train[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafb742",
   "metadata": {},
   "source": [
    "Putting together the text vector with the label index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7b448b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_ZipDataset element_spec=(TensorSpec(shape=(120,), dtype=tf.int32, name=None), TensorSpec(shape=(12,), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Create tf.data.Dataset objects from RaggedTensors and label tensors\n",
    "train_dataset_vectorized = tf.data.Dataset.zip((train_sequences, tf.data.Dataset.from_tensor_slices(y_train)))\n",
    "test_dataset_vectorized = tf.data.Dataset.zip((test_sequences, tf.data.Dataset.from_tensor_slices(y_test)))\n",
    "\n",
    "print(train_dataset_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da8c7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the datasets for training and testing\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "PREFETCH_SIZE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = (train_dataset_vectorized\n",
    "                 .cache()\n",
    "                 .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "                 .batch(BATCH_SIZE)\n",
    "                 .prefetch(PREFETCH_SIZE)\n",
    ")\n",
    "test_dataset = (test_dataset_vectorized\n",
    "                .cache()\n",
    "                .batch(BATCH_SIZE)\n",
    "                .prefetch(PREFETCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3a476",
   "metadata": {},
   "source": [
    "Save the tf datasets and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4d95a237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets and label arrays saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Remove existing folders if they exist\n",
    "train_dataset_path = os.path.join(current_dir, '..', 'data/processed', 'train_dataset')\n",
    "test_dataset_path = os.path.join(current_dir, '..', 'data/processed', 'test_dataset')\n",
    "\n",
    "if os.path.exists(train_dataset_path):\n",
    "    shutil.rmtree(train_dataset_path)\n",
    "if os.path.exists(test_dataset_path):\n",
    "    shutil.rmtree(test_dataset_path)\n",
    "\n",
    "# Save tf.data.Dataset objects\n",
    "tf.data.Dataset.save(train_dataset, train_dataset_path)\n",
    "tf.data.Dataset.save(test_dataset, test_dataset_path)\n",
    "\n",
    "print(\"Datasets and label arrays saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7cdec6",
   "metadata": {},
   "source": [
    "### 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3343ebb",
   "metadata": {},
   "source": [
    "### 5. Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ee20c",
   "metadata": {},
   "source": [
    "### 6. Next Steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
